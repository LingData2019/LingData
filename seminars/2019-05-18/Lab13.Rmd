---
title: "Lab 13. PCA and MCA"
output:
  html_document:
    df_print: paged
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE)
```

```{r}
library(tidyverse)
library(ggfortify)
Sys.setlocale(locale = "ru_RU.UTF-8")
```

## Principal component analysis (PCA)

### 1 Gospels' frequency word lists 

The gospels of Matthew, Mark, and Luke are referred to as the Synoptic Gospels and stand in contrast to John, whose content is comparatively distinct. This dataset (https://tinyurl.com/y8tcf3uw) contains frequency of selected words (without stopwords, without pronouns and without frequent word "Jesus") as attested in four gospels of the New Testament.

For some visualisations you will need assign row names to the dataframe:

```{r}
gospels <- read.csv("https://tinyurl.com/y8tcf3uw")
row.names(gospels) <- gospels$word
```

#### 1.1 Apply PCA to four continuous variables. What is the cumulative proportion of explained variance for the first and second component?

```{r}
PCA <- prcomp(gospels[,2:5], center = TRUE, scale. = TRUE)
summary(PCA)
```

#### 1.2 Use the `autoplot()` function of the library ggfortify for creating plot like this. 
See more examples here: https://cran.r-project.org/web/packages/ggfortify/vignettes/plot_pca.html

```{r}
autoplot(PCA,
         shape = FALSE,
         loadings = TRUE,
         label = TRUE,
         loadings.label = TRUE)+
  theme_bw()
```

#### 1.3 Predict the coordinates for the word "Jesus", which have the following frequencies: John = 0.05, Luke = 0.01, Mark = 0.02, Matthew = 0.02.

```{r}
predict(PCA, data.frame(John = 0.05, Luke = 0.01, Mark = 0.02, Matthew = 0.02))
```


### 2. Register variation in the British National Corpus

Dataset and discription from Natalia Levshina’s package Rling. 

This is a data set with relative frequencies (proportions) of different word classes in 69 subcorpora of the British National Corpus (the BYU-BNC version).
    Reg — a factor that describes the metaregister with levels Acad, Fiction, Misc, News, NonacProse and Spok
    Ncomm — a numeric vector with relative frequencies of common nouns.
    Nprop — a numeric vector with relative frequencies of proper nouns.
    Vpres — a numeric vector with relative frequencies of verbs in the present tense form, 3rd person singular.
    Vpast — a numeric vector with relative frequencies of verbs in the past tense form.
    P1 — a numeric vector with relative frequencies of the first-person pronouns.
    P2 — a numeric vector with relative frequencies of the second-person pronouns.
    Adj — a numeric vector with relative frequencies of adjectives.
    ConjCoord — a numeric vector with relative frequencies of coordinating conjunctions.
    ConjSub — a numeric vectorwith relative frequencies of subordinating conjunctions.
    Interject — a numeric vector with relative frequencies of interjections.
    Num — a numeric vector with relative frequencies of numerals.

2.1 Apply PCA to all variables. What is the cumulative proportion of explained variance for the first, second and third components?

2.2 Extract the coordinates from the pca object (pca$x), merge with the dataset itself, and create a visualization using the first two components and creating confidence ellipses for each metaregister.

```{r}
reg_bnc <- read.csv("https://goo.gl/19QywL")
pca <- prcomp(reg_bnc[,-1], center = TRUE, scale. = TRUE)
summary(pca)
autoplot(pca,
         shape = FALSE,
         loadings = TRUE,
         label = TRUE,
         loadings.label = TRUE)+
  theme_bw()
reg_bnc <- cbind(reg_bnc, pca$x)

reg_bnc %>% 
  ggplot(aes(PC1, PC2, color = Reg))+
  geom_point()+
  stat_ellipse()+
  theme_bw()
```

## Correspondence analysis (for categorical variables), CA

```{r, message=FALSE}
library(ca) # for Correspondence Analysis (Greenacre & Nenadic)
library("FactoMineR")
library("factoextra")
library(ggplot2) # to visualize data
library(reshape2) # to melt data in long format
```

### 3. Colour adjctives used by the Russian poets

#### 3.1. Read and preprocess data.

```{r}
silver <- read.csv("https://raw.githubusercontent.com/olesar/Reproducible-Research/master/freq_poet/authors_Acolor2_dataset.txt", sep="\t")
rownames(silver) <- silver[,1]
#silver <- silver[,-c(7,9)] # remove outliers
auth_nouns.long <- melt(silver, id.vars = "Lemma",  variable.name = "Author", value.name = "F")
#auth_nouns.long <- melt(silver, id.vars = "PoS",  variable.name = "Author", value.name = "F")
auth_nouns <- silver[,-1]
```


#### 3.2 Fit a CA model with FactoMineR.

```{r}
res.ca <- CA (auth_nouns[,-c(6,8)], 
              graph = FALSE)  
```

#### 3.3. Create a biplot:

```{r}
fviz_ca_row(res.ca) # plot rows
fviz_ca_col(res.ca) # plot columns
# biplot:
fviz_ca_biplot(res.ca, 
             col.row = "#00a35f",
             col.col = "#7a00a3",
             repel = TRUE,                                       # avoid overlap
             title = "")
```

#### 3.4. Print coordinates:

```{r}
row <- get_ca_row(res.ca)
row$coord[,1:2]
```

#### 3.5 Fit a CA model with supplementary data

```{r}
res.ca.sup <- CA (auth_nouns[,-c(6,8)], 
              col.sup = 7,
              graph = FALSE)  

# the same with supplementary rows, e.g. rows 15,16,17
#              row.sup = 15:18, 
fviz_ca_biplot(res.ca.sup, 
             col.row = "#00a35f",
             col.col = "#7a00a3",
             repel = TRUE,                                       # avoid overlap
             title = "")

```

#### 3.6 Quality of representation.

We can color the points by cos2 to show how good the plotted dimensions represent the "distance" between them.

```{r}
# Cos2, the quality of representation of rows on Dim.1 and Dim.2
fviz_ca_row(res.ca.sup, col.row = "cos2",
#             gradient.cols = c("#00a35f", "#003ca3", "#7a00a3"), 
             repel = TRUE,                                       # avoid overlap
             title = "")                                         
fviz_cos2(res.ca, choice = "row", axes = 1:2)

```

